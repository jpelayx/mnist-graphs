{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6oLZmENYINqH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision \n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage.segmentation import slic\n",
        "import skimage as ski\n",
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mnist_slic\n",
        "import model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8HQh8qJkKnC"
      },
      "source": [
        "Example for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_LpGX31tkKnD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = datasets.FashionMNIST(root = \"./fashion_mnist/test\", train=False, download=True, transform=T.ToTensor())\n",
        "len(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from compute_features import grayscale_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff91456c3d0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "img, y = ds[2]\n",
        "_, dim0, dim1 = img.shape\n",
        "img_np = img.view(dim0, dim1).numpy()\n",
        "n_segments = 20\n",
        "compactness = 0.1\n",
        "s, features = grayscale_features(img_np, n_segments, compactness)\n",
        "# s = slic(img_np, n_segments, compactness)\n",
        "print(len(np.unique(s)))\n",
        "plt.imshow(ski.segmentation.mark_boundaries(img_np, s, mode='subpixel'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.22021475, 0.37558958, 0.00840336, 0.02745098, 0.00692959,\n",
              "       0.02067143, 0.32941176])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features[0,:]/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 tensor(3)\n"
          ]
        }
      ],
      "source": [
        "ds = ConcatDataset([ds_train, ds_test])\n",
        "y = torch.cat([ds_train.targets, ds_test.targets])\n",
        "print(ds[idx][1], y[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=5)\n",
        "skf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_segments = 10\n",
        "compactness = 0.1\n",
        "\n",
        "# features \n",
        "get_avg_color = True\n",
        "get_std_deviation_color = True\n",
        "get_centroid = True\n",
        "get_std_deviation_centroid = True\n",
        "get_num_pixels = True\n",
        "get_avg_color_distance = False\n",
        "get_std_dev_color_distance = False\n",
        "\n",
        "feature_mask = np.array([True, True, True, True, True, True, False])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "(5, 5)\n"
          ]
        }
      ],
      "source": [
        "img, y = ds[0]\n",
        "_, dim0, dim1 = img.shape\n",
        "img_np = img.view(dim0, dim1).numpy()\n",
        "s, features, edges = grayscale_features(img_np, n_segments, compactness)\n",
        "print(len(np.unique(s)))\n",
        "x = features.T[feature_mask]\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 5)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# same as function used in SuperPixelGraphMNIST\n",
        "img, y = ds[1]\n",
        "_, dim0, dim1 = img.shape\n",
        "img_np = img.view(dim0, dim1).numpy()\n",
        "s, features, edges = grayscale_features(img_np, n_segments, compactness)\n",
        "g = ski.future.graph.rag_mean_color(img_np, s)\n",
        "n = g.number_of_nodes()\n",
        "s1 = np.zeros([n, 1])  # for mean color and std deviation\n",
        "s2 = np.zeros([n, 1])  # for std deviation\n",
        "pos1 = np.zeros([n, 2]) # for centroid\n",
        "pos2 = np.zeros([n, 2]) # for centroid std deviation\n",
        "num_pixels = np.zeros([n, 1])\n",
        "for idx in range(dim0 * dim1):\n",
        "        idx_i, idx_j = idx % dim0, int(idx / dim0)\n",
        "        node = s[idx_i][idx_j] - 1\n",
        "        s1[node][0]  += img_np[idx_i][idx_j]\n",
        "        s2[node][0]  += pow(img_np[idx_i][idx_j], 2)\n",
        "        pos1[node][0] += idx_i\n",
        "        pos1[node][1] += idx_j\n",
        "        pos2[node][0] += pow(idx_i, 2)\n",
        "        pos2[node][1] += pow(idx_j, 2)\n",
        "        num_pixels[node][0] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# computing features\n",
        "edge_index = torch.from_numpy(np.array(g.edges).T).to(torch.long)\n",
        "x = []\n",
        "s1 = s1/num_pixels\n",
        "avg_color = s1\n",
        "if get_avg_color:\n",
        "    x.append(torch.from_numpy(avg_color.flatten()).to(torch.float))\n",
        "s2 = s2/num_pixels\n",
        "std_deviation = np.sqrt(s2 - s1*s1)\n",
        "if get_std_deviation_color:\n",
        "    x.append(torch.from_numpy(std_deviation.flatten()).to(torch.float))\n",
        "pos1 = pos1/num_pixels\n",
        "pos = torch.from_numpy(pos1).to(torch.float)\n",
        "if get_centroid:\n",
        "    x.append(pos[:,0])\n",
        "    x.append(pos[:,1])\n",
        "if get_std_deviation_centroid:\n",
        "    pos2 = pos2/num_pixels\n",
        "    std_deviation_centroid = torch.from_numpy(np.abs(np.sqrt(pos2 - pos1*pos1))).to(torch.float)\n",
        "    x.append(std_deviation_centroid[:,0])\n",
        "    x.append(std_deviation_centroid[:,1])\n",
        "if get_num_pixels:\n",
        "    x.append(torch.from_numpy(num_pixels.flatten()).to(torch.float))\n",
        "if get_avg_color_distance or get_std_dev_color_distance:\n",
        "    distances = [[g.edges[u,v]['weight'] for u, v in g.edges(node_idx)] for node_idx in range(n)]\n",
        "    if get_avg_color_distance:\n",
        "        x.append(torch.Tensor([np.average(distance) for distance in distances]))\n",
        "    if get_std_dev_color_distance:\n",
        "        x.append(torch.Tensor([np.std(distance) for distance in distances]))\n",
        "data = Data(x=torch.stack(x, dim=1), edge_index=edge_index, pos=pos, y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "(7, 5)\n"
          ]
        }
      ],
      "source": [
        "print( len(x))\n",
        "print(features.T.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "print(f'Grph with {g.number_of_nodes()} nodes and {g.number_of_edges()} edges')\n",
        "print(f'Label: {data.y}')\n",
        "color_feature = 0\n",
        "pos = dict(zip(range(data.num_nodes), data.pos.numpy()))\n",
        "nx.draw(g, pos=pos, node_color=data.x[:,color_feature])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aa29a3bab474a02941d4f6b5bb2e821",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/train-images-idx3-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1856fd72077a47b7b451aa181224b867",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/train-labels-idx1-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a894e802ab3244ddb0599dc6e5e5934a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/t10k-images-idx3-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a10f2d7a7458496789d4b4a80a7a9503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw/t10k-labels-idx1-ubyte.gz to test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/julia/Documents/cic/petwin/superpixel-gnns/mnist_slic_viz.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/julia/Documents/cic/petwin/superpixel-gnns/mnist_slic_viz.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spds \u001b[39m=\u001b[39m mnist_slic\u001b[39m.\u001b[39;49mSuperPixelGraphMNIST(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m~/Documents/cic/petwin/superpixel-gnns/mnist_slic.py:35\u001b[0m, in \u001b[0;36mSuperPixelGraphMNIST.__init__\u001b[0;34m(self, root, n_segments, compactness, features, train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m get_ds_name(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_segments, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompactness, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain) \u001b[39mif\u001b[39;00m root \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m root\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_pre_loaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m])\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_stats()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:50\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m              transform: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m              pre_transform: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m              pre_filter: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:87\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/data/dataset.py:170\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m    169\u001b[0m makedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[1;32m    172\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39m'\u001b[39m\u001b[39mpre_transform.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    173\u001b[0m torch\u001b[39m.\u001b[39msave(_repr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform), path)\n",
            "File \u001b[0;32m~/Documents/cic/petwin/superpixel-gnns/mnist_slic.py:156\u001b[0m, in \u001b[0;36mSuperPixelGraphMNIST.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 156\u001b[0m     data, slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloadMNIST()\n\u001b[1;32m    157\u001b[0m     torch\u001b[39m.\u001b[39msave((data, slices), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m])\n",
            "File \u001b[0;32m~/Documents/cic/petwin/superpixel-gnns/mnist_slic.py:71\u001b[0m, in \u001b[0;36mSuperPixelGraphMNIST.loadMNIST\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloadMNIST\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_pre_loaded \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     mnist \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mMNIST(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, train\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49mT\u001b[39m.\u001b[39;49mToTensor())\n\u001b[1;32m     72\u001b[0m     img_total \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     73\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mimg_total\u001b[39m}\u001b[39;00m\u001b[39m images with n_segments = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_segments\u001b[39m}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:104\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_exists():\n\u001b[1;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_data()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:123\u001b[0m, in \u001b[0;36mMNIST._load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    122\u001b[0m     image_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mt10k\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-images-idx3-ubyte\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m     data \u001b[39m=\u001b[39m read_image_file(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_folder, image_file))\n\u001b[1;32m    125\u001b[0m     label_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mt10k\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-labels-idx1-ubyte\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     targets \u001b[39m=\u001b[39m read_label_file(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_folder, label_file))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:544\u001b[0m, in \u001b[0;36mread_image_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_image_file\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 544\u001b[0m     x \u001b[39m=\u001b[39m read_sn3_pascalvincent_tensor(path, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    545\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39muint8:\n\u001b[1;32m    546\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx should be of dtype torch.uint8 instead of \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:526\u001b[0m, in \u001b[0;36mread_sn3_pascalvincent_tensor\u001b[0;34m(path, strict)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39m# The MNIST format uses the big endian byte order. If the system uses little endian byte order by default,\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m# we need to reverse the bytes before we can read them with torch.frombuffer().\u001b[39;00m\n\u001b[1;32m    525\u001b[0m needs_byte_reversal \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mbyteorder \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlittle\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m num_bytes_per_value \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 526\u001b[0m parsed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrombuffer(\u001b[39mbytearray\u001b[39;49m(data), dtype\u001b[39m=\u001b[39mtorch_type, offset\u001b[39m=\u001b[39m(\u001b[39m4\u001b[39m \u001b[39m*\u001b[39m (nd \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m needs_byte_reversal:\n\u001b[1;32m    528\u001b[0m     parsed \u001b[39m=\u001b[39m parsed\u001b[39m.\u001b[39mflip(\u001b[39m0\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "spds = mnist_slic.SuperPixelGraphMNIST(root='test-n75-c0.1-avg_color-std_deviation_color-centroid-std_deviation_centrtoid-avg_color_distance-std_deviation_color_distance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "data = spds[3]\n",
        "\n",
        "g = to_networkx(data, to_undirected=True)\n",
        "print(f'Grph with {g.number_of_nodes()} nodes and {g.number_of_edges()} edges')\n",
        "print(f'Label: {data.y[0]}')\n",
        "color_feature = 4\n",
        "pos = dict(zip(range(data.num_nodes), data.pos.numpy()))\n",
        "nx.draw(g, pos=pos, node_color=data.x[:,color_feature])\n",
        "# acho que ta virado ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.num_node_features"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
